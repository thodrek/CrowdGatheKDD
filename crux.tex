% THIS IS AN EXAMPLE DOCUMENT FOR VLDB 2012
% based on ACM SIGPROC-SP.TEX VERSION 2.7
% Modified by  Gerald Weber <gerald@cs.auckland.ac.nz>
% Removed the requirement to include *bbl file in here. (AhmetSacan, Sep2012)
% Fixed the equation on page 3 to prevent line overflow. (AhmetSacan, Sep2012)
\listfiles
%\documentclass{sig-alternate}
\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{color}
\usepackage{balance}  % for  \balance command ON LAST PAGE  (only there!)
\usepackage{times}
\usepackage{url}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{subfigure}
\usepackage{xspace}
\usepackage[noend]{algorithmic}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{cleveref}
\usepackage{soul}
%\usepackage[font={small,it}]{caption}

\newcommand{\squishlist}{
   \begin{list}{$\bullet$}
    {
      \setlength{\itemsep}{0pt}
      \setlength{\parsep}{3pt}
      \setlength{\topsep}{3pt}
      \setlength{\partopsep}{0pt}
      \setlength{\leftmargin}{1.5em}
      \setlength{\labelwidth}{1em}
      \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
    \end{list}  }

\renewcommand*\ttdefault{cmvtt}

%\makeatletter
%\def\@copyrightspace{\relax}
%\makeatother


\newcommand{\argmax}{\operatornamewithlimits{arg\ max}}

\newcommand{\eat}[1]{}
\newcommand{\todo}[1]{\textcolor{red}{{TODO: #1}}}
\newcommand{\add}[1]{\textcolor{red}{{ADD: #1}}}
\newcommand{\note}[1]{\textcolor{blue}{{#1}}}
\newcommand{\agp}[1]{\textcolor{blue}{{Aditya: #1}}}
\newcommand{\stitle}[1]{\vspace{0.5em}\noindent\textbf{#1}}


\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{problem}{Problem}
\newtheorem{reduction}{Reduction}
\newcommand{\domain}{\mathcal{D}}
\newcommand{\attributes}{\mathcal{A}_D}
\newcommand{\hierarchy}{\mathcal{H}_D}
\newcommand{\attrhierarchy}{\mathcal{H}_A}
\newcommand{\workers}{\mathcal{W}}
\newcommand{\uentities}{\mathcal{E}}
\newcommand{\queryvector}{{\bf Q_S}}

\newif\ifpaper
\papertrue % comment out to create technical report

\newif\iftr
%\trtrue % uncomment to create technical report



\begin{document}

% ****************** TITLE ****************************************

%\title{CrowdGather: Entity Extraction over Structured Domains}
\title{CRUX: Crowdsourced Data Extraction from the Long Tail}
%\numberofauthors{3} 
%
%\author{
%	\alignauthor Theodoros Rekatsinas\\
%            \affaddr{University of Maryland, College Park} 
%                \email{thodrek@cs.umd.edu}
%            \alignauthor Amol Deshpande\\
%            \affaddr{University of Maryland, College Park} 
%                \email{amol@cs.umd.edu}
%            \alignauthor Aditya Parameswaran \\
%            \affaddr{University of Illinois, Urbana-Champaign} 
%                \email{adityagp@illinois.edu}
%}

\author{\IEEEauthorblockN{Theodoros Rekatsinas}
\IEEEauthorblockA{Stanford University\\
Email: thodrek@cs.stanford.edu}
\and
\IEEEauthorblockN{Amol Deshpande}
\IEEEauthorblockA{University of Maryland, College Park\\
Email: amol@cs.umd.edu}
\and
\IEEEauthorblockN{Aditya Parameswaran}
\IEEEauthorblockA{University of Illinois, Urbana-Champaign\\
Email: adityagp@illinois.edu}}

\maketitle

\begin{abstract}
Extracting and collecting information about real-world entities, such as people, organizations, and real-world events, in structured data repositories has enhanced our ability to interpret and understand content and offer personalized services. However, existing repositories suffer from a lack of fine-grained {\em long tail} data, i.e., the not-so-popular entities. In this paper, we examine how crowdsourcing techniques can be leveraged for entity extraction, thereby dramatically increasing coverage. We leverage two novel insights: {\em exploiting the structure of the domain of interest}, and {\em using exclude list to limit the repeated extractions}. We apply these insights in developing CRUX, a CRowdsoUrced data eXtraction framework that optimizes crowdsourced entity extraction by dynamically adapting the queries issued against the crowd to minimize the number of duplicate extractions, and thereby the wasted cost. We develop new statistical tools to reason about the gain, i.e., the number of new distinct extracted entities, of issuing {\em further queries} under the presence of little to no information; this allows us to effectively prune futile queries. We also introduce algorithms to discover adaptive querying strategies that maximize the number of distinct extracted entities under budget constraints. We evaluate our techniques on both synthetic and real-world datasets, demonstrating an improvement of up to 300\% over competing approaches for the same budget.
\end{abstract}

\input{sections/intro_new}
\input{sections/prelims}
\input{sections/problem}
\input{sections/gainestimators}
\input{sections/solving}
\input{sections/exps}
\input{sections/related}
\input{sections/conclusions}
\balance

%\ifpaper
%\scriptsize
%\fi
{\small 
\bibliographystyle{abbrv}
\bibliography{crux}
}
\end{document}
