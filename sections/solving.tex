%!TEX root = ../crowd_hierarchies_kdd.tex


\section{Discovering Querying Policies}
\label{sec:solving}
We now turn our attention to the core component of CRUX and introduce a heuristic multi-round adaptive optimization algorithm for identifying querying strategies that maximize the total gain across all rounds under the given budget constraints. At each round we assume oracle access to a gain estimator for any generalized query $q^v(k,E)$. Such oracles can be constructed using the techniques introduced in the previous section. The gain of each query can be viewed as a random variable. By issuing a query we get to observe the value of this random variable and using the previous observations we need to decide which query to issue. Given the above, our framework builds upon ideas from the multi-armed bandit literature~\cite{Auer:2003,EvenDar06actionelimination}. Before presenting our framework we discuss some challenges associated with this adaptive optimization problem.

\squishlist
\item The first challenge is that the number of nodes in $\hierarchy$ is exponential in the number of attributes $\attributes$ describing the domain of interest. Querying every possible node to estimate its expected return for different queries $q^v(k,E)$ is prohibitively expensive. That said, typical budgets do not allow algorithms to query all nodes in the hierarchy, so this intractability may not hurt us all that much. For example, we keep estimates for each of the nodes for which at least one entity has been retrieved.
\item The second challenge is balancing the tradeoff between {\em exploitation} and {\em exploration}~\cite{Auer:2003}. The first refers to querying nodes for which sufficient entities have been retrieved, and hence, we have an accurate estimate for their expected return; the latter refers to exploring new nodes in $\hierarchy$ to avoid locally optimal policies.
\item The last challenge is optimizing over the exclude list to be used in generalized queries. Optimizing over all potential exclude lists (i.e., considering all potential subsets of observed entities to be added in the list) leads to an exponential explosion of the query space.
\squishend

\subsection{Optimizing over Query Configurations}
\label{sec:config}
Instead of optimizing over all potential queries $q^v(k,E)$ that can be issued at the different nodes of $\hierarchy$, we optimize over all potential query configurations $(k,l,v)$. In particular, we do not optimize directly for the exclude list to be used in a further query but rather for the size $l$ of it. Once we decide on $l$ the exclude list $E$ can be constructed following a randomized approach, where $l$ of the retrieved entities are included in the list uniformly at random. The generated list is used to update the frequency counts $f_i$ and sample size $n$ and estimate the gain of the query. Bootstrapping is also used to obtain improved estimates. 

We follow a randomized approach since a deterministic construction of $E$ that picks the {\em l-most} popular items in the running sample is very sensitive to the {\em observed popularity distribution}. When the number of observed entities corresponds to a small portion of the entire population - as in the scenarios we consider in this paper - the individual entity popularity estimates tend to be noisy.  We empirically observed that a deterministic construction of a limited size exclude list, especially during early queries, leads to poor popularity estimates. Thus, we follow a randomized approach. Below we denote a query configuration $(k,l,v)$ and a query $q^v(k,E)$ with $|E| = l$ using the same symbol $q$ for convenience. Given $l$ we construct $E$ following the aforementioned approach.

Given the above, we need to estimate the gain for a query configuration $(k,l,v)$ instead of a query $q^v(k,E)$. When $E = \emptyset$ then we can directly use the techniques introduced in \Cref{sec:gainestimators} techniques introduced in \Cref{} 

\subsection{Balancing Exploration and Exploitation}
\label{sec:balancing}
When issuing queries $q^v(k,E)$ at different nodes of $\hierarchy$ we obtain a collection of entities that can be assigned to different nodes in $\hierarchy$. For each node we can estimate the gain of a query $q^v(k,E)$ corresponding to a configuration $(k,l,v)$ with $|E| =  l$ using the estimators presented in \Cref{sec:gainestimators}. However, this estimate is based on a rather small sample of the underlying population. Thus, exploiting this information at every round may lead to suboptimal decisions. Because of this we need to balance the trade-off between exploiting queries for which the estimated gain is high and queries that have not been selected many times. Formally, the latter corresponds to upper-bounding the expected gain of each potential query with a {\em confidence interval} that depends on both the variance of the expected gain and the number of times a query has been issued~\cite{Auer:2003}.

Let $r(q)$ denote the expected gain of query $q^v(k,E)$ obtained by one of techniques in \Cref{sec:gainestimators}. This is an estimate of the true return $r^*(q)$. Moreover, let $\sigma(q)$ be an error component on the gain of query $q$ chosen such that $r(q) - \sigma(q) \leq r^*(q) \leq r(q) + \sigma(q)$ with high probability. The parameter $\sigma(q)$ should takes into account both the {\bf empirical variance} of the expected gain as well as our {\bf uncertainty} about the gain of query $q$ if $q$ has been chosen only a few times. Given the quantities $r(q)$ and $\sigma(q)$, we assign a score to each query by considering a linear function of the quantity $r(q) + \sigma(q)$. This score prioritizes exploration when the variance is or our uncertainty is high, and thus, we could potentially discover a profitable query, and exploitation when the estimated gain is high. Next, we discuss how we set the quantity $\sigma(q)$.

We proceed in rounds and at each round select a query. Let $n_{q,t}$ be the number of times we have chosen query $q$ by round $t$, and let $v_{q,t}$ denote the maximum value between some constant (e.g., $0.01$) and the empirical variance for the gain of query $q$ at round $t$. The latter can be computed using bootstrapping over the retrieved sample and applying the estimators presented in \Cref{sec:gainestimators} over these bootstrapped samples. Several techniques have been proposed in the multi-armed bandits literature to compute the parameter $\sigma(q)$~\cite{teytaud:inria-00173263}. Teytaud et al.~\cite{teytaud:inria-00173263} showed that techniques considering both the variance and the number of times an action has been chosen tend to outperform other proposed methods. Based on this observation, we choose to use the following formula for sigma:
\begin{equation}
\label{eq:upper}
\sigma(q,t) = \sqrt{\frac{v_{q,t}\cdot\log(t)}{n_{q,t}}}
\end{equation}

\subsection{A Multi-Round Querying Policy Algorithm}
\label{sec:heuristic}
We now introduce a multi-round algorithm for solving the budgeted entity enumeration problem. Instead of optimizing over all potential queries $q^v(k,E)$ that can be issued at the different nodes of $\hierarchy$, we optimize over all potential query configurations $(k,l,v)$. In particular, we do not optimize directly for the exclude list to be used in a further query but rather for the size $l$ of it. Once we decide on $l$ the exclude list $E$ can be constructed following a randomized approach, where $l$ of the retrieved entities are included in the list uniformly at random. The generated list is used to update the frequency counts $f_i$ and sample size $n$ and estimate the gain of the query. Bootstrapping is also used to obtain improved estimates. 

We follow a randomized approach since a deterministic construction of $E$ that picks the {\em l-most} popular items in the running sample is very sensitive to the {\em observed popularity distribution}. When the number of observed entities corresponds to a small portion of the entire population - as in the scenarios we consider in this paper - the individual entity popularity estimates tend to be noisy.  We empirically observed that a deterministic construction of a limited size exclude list, especially during early queries, leads to poor popularity estimates. Thus, we follow a randomized approach. Below we denote a query configuration $(k,l,v)$ and a query $q^v(k,E)$ with $|E| = l$ using the same symbol $q$ for convenience. Given $l$ we construct $E$ following the aforementioned approach.

Given the above the gain estimation techniques introduced in \Cref{} 

\begin{algorithm}[h]
\small\caption{Multi-round Extraction Algorithm}
\label{algo:overall}
\begin{algorithmic}[1]
\STATE {\bf Input:} $\hierarchy$: the hierarchy describing the entity domain; $K$: set of valid query size assignments; $L$: set of valid exclude-list size assignments; $\tau_c$: query budget; $r,\sigma$: value oracle access to upper-bounded gain for different query configurations; $c$: value oracle access to the query configuration costs;
\STATE {\bf Output:} $\uentities$: a set of extracted distinct entities;
\STATE $\uentities \leftarrow \{\}$
\STATE $t \leftarrow 1$ /* Initialize round counter */
\STATE ${\sf Rembudget} \leftarrow \beta_c$ /* Initialize remaining budget */
\STATE $\mathcal{Q} \leftarrow$ {\sf ActiveQueryConf($\hierarchy$, $\emptyset$, NULL $K$, $L$, $t$)} /* Initialize active query configurations */
\WHILE {${\sf Rembudget} > 0$ and $\mathcal{Q} \neq \{\}$}
	\STATE $q^* \leftarrow \arg\max_{q \in {\mathcal{Q}}} \frac{r(q)+\sigma(q,t)}{c(q)}$ such that ${\sf Rembudget} - c(q^*) >0$
	\IF {$q$ is NULL }
		\STATE break;
	\ENDIF
	\STATE ${\sf Rembudget} \leftarrow {\sf Rembudget} - c(q^*)$ /* Update budget */
	\STATE Issue query $q^*$; 
	\STATE $v^* \leftarrow $ The node for $q^*$;
	\STATE $E \leftarrow$ entities extracted by query $q^*$
	\STATE $\uentities \leftarrow \uentities \cup E$ /* Update unique entities */
	\STATE $\mathcal{Q} \leftarrow$ {\sf ActiveQuerySet($\hierarchy$, $\mathcal{Q}$, $v^*$, $K$, $L$, $t$)} /* Update active queries */
	\STATE $t \leftarrow t + 1$ /* Increase round counter */
\ENDWHILE
\RETURN $\uentities$
\end{algorithmic}
\end{algorithm}

An overview of our multi-round optimization algorithm is shown in Algorithm~\ref{algo:overall}. The algorithm takes as input the poset $\hierarchy$ describing the extraction domain, a set $K$ of valid query size assignments, a set $L$ of valid exclude list size assignments and a budget $\tau_c$. The algorithm also assumes access to an oracle providing the values $r(q)$ and $\sigma(q,t)$ - $t$ denotes the round count -  which characterize the upper gain of a query $q$ and an oracle providing the cost $c(q)$ of a query $q$. Finally, the algorithm has access to the method ${\sf ActiveQuerySet}$ (see \Cref{sec:badactions}) which determines the possible queries to be considered at each round.

Our multi-round extraction algorithm proceeds as follows: First it initializes the set $\uentities$ of extracted entities, the round count $t$, the remaining budget ${\sf Rembudget}$ and the set of candidate queries $\mathcal{Q}$ (i.e., queries to be considered) at the first round (Ln.3-6). Then it proceeds in rounds and iteratively selects one query to be issued against the crowd until the total budget is utilized or the set of candidate queries is empty (Ln. 7). At each round our algorithm performs the following steps. It first detects a query in $\mathcal{Q}$ that maximizes the score quantity $\frac{r(q) + \sigma(q,t)}{c(q)}$ under the constraint that the cost of query $q$ is less or equal to the remaining budget (Ln. 8). If no such query exists the algorithm terminates (Ln. 9-10). Otherwise, the algorithm proceeds by executing the selected query, updating the set of extracted entities, the remaining budget, the set of active queries and round counter (Ln. 11 - 17). Finally, Algorithm 1 returns the set of distinct entities (Ln. 18). 

\vspace{2pt}\noindent\textbf{Discussion.} The reason, we chose quantity $\frac{r(q) + \sigma(q,t)}{c(q)}$ as the assigned query score is due the presence of a total budget and the connection between the problem of budgeted crowdsourced entity extraction and knapsack. Finally, method ${\sf ActiveQuerySet}$ is used to restrict the set of candidate queries considered by our algorithm. As discussed before the size of $\hierarchy$ is exponential to the values of attributes describing it, and thus, considering all possible queries for the different nodes of $\hierarchy$ can be prohibitively expensive. 

\subsection{Updating the Set of Actions}
\label{sec:badactions}
We now introduce an algorithm for effectively limiting the number of queries at each round of Algorithm \ref{algo:overall}. To do so we exploit the structure poset $\hierarchy$ and the gain estimates for different query configurations $(k,l,v)$.

Let $\mathcal{Q}$ denote the set of candidate (or active) queries to be considered. We propose an algorithm that {\bf adds} queries to $\mathcal{Q}$ by traversing the input poset in a top-down manner and adds new actions that correspond to queries for nodes that are {\em direct descendants} of already queried nodes. 

The reason we adopt a top-down traversal is two-fold: (i) Due to the hierarchical structure of the poset nodes at higher levels of the poset correspond to larger populations of entities. Therefore, issuing queries at these nodes can potentially result in a larger number of extracted entities. Hence, traversing the poset in a top-down manner allows us to detect sparsely populated areas of the poset and prune futile queries. (ii)According to our cost model assumption, issuing queries at higher-level nodes (i.e., less specialized queries) is cheaper. Therefore, the gain to cost ratio can be larger for queries that correspond to higher-level nodes of the poset.

As mentioned above the number of nodes in $\hierarchy$ can be prohibitively large, therefore we also {\em remove} any {\em bad queries} from set $\mathcal{Q}$. A query $q$ is is bad at round $t$ when $r(q) + \sigma(q,t) < \max_{q^{\prime} \in \mathcal{Q}} (r(q^{\prime}) - \sigma(q^{\prime},t))$. Intuitively, this states that we do not need to consider a query as long as there exists another query such that the upper-bounded gain of the former is lower than the lower bounded gain of the latter. This technique is also adopted in multi-armed bandits to limit the number of actions considered by the algorithm~\cite{EvenDar06actionelimination}. 


An overview of our algorithm is shown in Algorithm~\ref{algo:updateactions}. This also implements the method ${\sf ActiveQuerySet}$ introduced above. The algorithm takes as input the poset $\hierarchy$ describing the extraction domain, a set $K$ of valid query size assignments, a set $L$ of valid exclude list size assignments and a budget $\tau_c$. The algorithm also assumes access to an oracle providing the values $r(q)$ and $\sigma(q,t)$ - $t$ denotes the round count -  which characterize the upper gain of a query $q$ and an oracle providing the cost $c(q)$ of a query $q$. Finally, the algorithm has access to the method ${\sf ActiveQuerySet}$ (see \Cref{sec:badactions}) which determines the possible queries to be considered at each round.

An overview of our algorithm for determining the set of active queries to be considered at each round is shown in Algorithm~\ref{algo:updateactions}. This algorithm implements the method ${\sf ActiveQuerySet}$ introduced above. The method takes as input
Our approach for updating the set of available actions (Alg.~\ref{algo:updateactions}) proceeds as follows: If the set of available actions is empty start by considering all possible queries that can be issued at the root of $\hierarchy$ (Ln. 4-5). The set of possible queries corresponds to queries $q(k,E)$ for all combinations of the values of parameters $k$ and $l$. Recall that $E$ is constructed in a randomized fashion once $l$ is determined. Recall that these are pre-specified by the designer of the querying interface. If the set of available actions is not empty, we consider the node associated with the action selected in the last round and populate the set of available actions with all the queries corresponding to its direct descendants (Ln. 7-9), i.e., by traversing the input poset in a bottom-down fashion. As mentioned above the number of nodes in $\hierarchy$ can be prohibitively large, therefore we also {\em remove} any {\em bad actions} from the running set of actions (Ln.  10-14). 
An action $\alpha$ is bad when $r(\alpha) + \sigma(\alpha) < \max_{\alpha^{\prime} \in \mathcal{S}} (r(\alpha^{\prime}) - \sigma(\alpha^{\prime}))$. Intuitively, this states that we do not need to consider an action as long as there exists another action such that the upper-bounded return of the former is lower than the lower bounded return of the latter. This is a standard technique adopted in multi-armed bandits to limit the number of actions considered by the algorithm~\cite{EvenDar06actionelimination}. 


\begin{algorithm}[h]
\small\caption{UpdateActionSet}
\label{algo:updateactions}
\begin{algorithmic}[1]
\STATE {\bf Input:} $\hierarchy$: the hierarchy describing the entity domain; $\mathcal{Q}$: the running set of active queries; $v^*$: the node in $\hierarchy$ associated with the last query; $K$: set of valid query size assignments; $L$: set of valid exclude-list size assignments; $t$: running round counter; $r,\sigma$: value oracle access to gain upper bound; 
\STATE {\bf Output:} $\mathcal{Q}_{new}$: the updated set of active queries;
\STATE \textbf{/* Extend Set of Active Queries*/}
\IF {$\mathcal{Q}$ is empty} 
	\STATE $\mathcal{Q}_{new} \leftarrow \{ \Cup_{k \in K \times l \in L} (k,l,root) \}$ 
\ENDIF 
\STATE $\mathcal{S}_{new} \leftarrow \mathcal{S}_{old}$
\FORALL{$d \in ${\sf ~Set of Direct Descendant Nodes of $u$ in $\hierarchy$}}
\STATE $A_d \leftarrow$ Set of queries at $u$ for all configurations in $V_k \times V_l$
\STATE $\mathcal{S}_{new} \leftarrow \mathcal{S}_{new} \cup A_d$
\ENDFOR
\STATE \textbf{/* Remove Bad Actions*/}
\STATE /* Find maximum lower bound on gain over all actions in $\mathcal{S}_{new}$*/
\STATE $thres \leftarrow \max_{\alpha^{\prime} \in \mathcal{S}_{new}} (r(\alpha^{\prime}) - \sigma(\alpha^{\prime}))$  
\STATE $\mathcal{B} \leftarrow$ All actions $a$ in $\mathcal{S}_{new}$ with $r(\alpha) + \sigma(\alpha) < thres$
\STATE $\mathcal{S}_{new} \leftarrow \mathcal{S}_{new} \setminus \mathcal{B}$
\RETURN $\mathcal{S}_{new}$
\end{algorithmic}
\end{algorithm}
\vspace{-5pt}
