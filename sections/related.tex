%!TEX root = ../crux-sigconf.tex


\section{Related Work}
\label{sec:related}
Prior work related to the techniques proposed in this paper can be placed in a few categories; we describe each of them in turn:
\iftr
\vspace{3pt}\noindent\textbf{Crowd Algorithms.} There has been a significant work on designing algorithms where the unit operations 
 (e.g., comparisons, predicate evaluations, and so on) 
 are performed by human workers, including common database primitives such as filter~\cite{crowdscreen}, join~\cite{markus-sorts-joins} and max~\cite{so-who-won},  machine learning primitives, such as entity resolution~\cite{ crowder} and clustering~\cite{crowdclustering}, as well as data mining primitives~\cite{amsterdamer:2013, get-another-label}. 
 \fi
We have already
discussed prior work on crowdsourced extraction or enumeration~\cite{park:2014, trushkowsky:2013} in the introduction. 
\iftr
In both cases, the focus is on a single entity extraction query; extracting entities from large and diverse data domains is not considered. Moreover, the proposed techniques do not support  dynamic querying strategies to optimize for a specified monetary budget. 
\fi

%Finally, to optimize the tradeoff between the gain and cost of queries  previous work proposes either a {\em pay-as-you-go} scheme~\cite{trushkowsky:2013} or a fixed answer size scheme~\cite{park:2014}. In the first case, one repeatedly issues queries to the crowd until the {\em marginal gain}, i.e., the difference between the new extracted entities and the querying cost, drops below a desired threshold. However, the proposed scheme does not enforce any budget constraints explicitly and focuses on a single query in isolation. Thus, it does not optimize the gain-cost tradeoff over an entire querying policy. In the second case, one repeatedly issues queries to the crowd until a desired number of entities is retrieved. The latter is specified by the user. Tthis assumes knowledge of the number of entities to be extracted, which may not be available in many real-world scenarios. 

\vspace{3pt}\noindent\textbf{Knowledge Acquisition Systems.} Recent work has also considered the problem of using crowdsourcing within knowledge acquisition systems~\cite{jiang:13, kondredi:2014, west:2014}. This line of work suggests using the crowd for curating knowledge bases 
\iftr (e.g., assessing the validity of the extracted facts) 
\fi and for gathering additional information to be added to the knowledge base\iftr (e.g., missing attributes of an entity or relationships between entities)\fi, instead of augmenting the set of entities themselves. 
\iftr As a result, these papers are solving an orthogonal problem. The techniques described in this paper for estimating the amount of information from a query and devising querying strategies to maximize the amount of extracted information will surely be beneficial for knowledge extraction systems as well.
\fi

\vspace{3pt}\noindent\textbf{Deep Web Crawling.} A different line of work has focused on data extraction from the deep web~\cite{Jin:2011,Sheng:2012} where data is obtained by querying a form-based interface over a hidden database and extracting results\iftr from dynamically-generated answers (often a list of entities)\fi. Sheng et al.~\cite{Sheng:2012} provide near-optimal algorithms that exploit the exposed structure of the underlying domain to extract all the tuples present in the hidden database. Our goal is similar in that we also extract entities via a collection of interfaces\iftr (in our case the interfaces correspond to queries asked to the crowd)
\fi. Unlike our setting, answers from a hidden database are deterministic, i.e., a query will always retrieve the same top-k tuples. So, it suffices to ask each query precisely once, making it much simpler.
\iftr 
In our setting, since crowdsourced entity extraction queries can be viewed as random samples from an unknown  distribution, one needs to make use of the query result estimation techniques from \Cref{sec:gainestimators}.
\fi